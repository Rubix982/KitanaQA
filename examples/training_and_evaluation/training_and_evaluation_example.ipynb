{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we show how a standard training and evaluation run can be integrated into an end-to-end Flow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy train and test data from package\n",
    "import pkg_resources\n",
    "TRAIN_PATH = pkg_resources.resource_filename(\n",
    "            'kitanaqa', 'support/unittest-squad.json')\n",
    "EVAL_PATH = pkg_resources.resource_filename(\n",
    "            'kitanaqa', 'support/unittest-squad.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:03 - kitanaqa - WARNING - Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "11/12/2020 18:35:03 - WARNING - kitanaqa -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "11/12/2020 18:35:03 - kitanaqa - INFO - Training/evaluation parameters TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_18-35-03_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True)\n",
      "11/12/2020 18:35:03 - INFO - kitanaqa -   Training/evaluation parameters TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_18-35-03_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True)\n",
      "11/12/2020 18:35:03 - INFO - filelock -   Lock 4820264552 acquired on ./example_outputs/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f31f222744640e192e4f712c3b95f5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:04 - INFO - filelock -   Lock 4820264552 released on ./example_outputs/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:05 - INFO - filelock -   Lock 4820353432 acquired on ./example_outputs/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff11e6d2bca4711ac1c7d884c1fdb0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=442.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:05 - INFO - filelock -   Lock 4820353432 released on ./example_outputs/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.8949e27aafafa845a18d98a0e3a88bc2d248bbc32a1b75947366664658f23b1c.lock\n",
      "11/12/2020 18:35:05 - INFO - filelock -   Lock 4820384680 acquired on ./example_outputs/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdbda4a48a44159b0b3b9c7b5c74068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=267967963.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:22 - INFO - filelock -   Lock 4820384680 released on ./example_outputs/ae9df7a8d658c4f3e1917a471a8a21cf678fa1d4cb91e7702dfe0598dbdcf354.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "11/12/2020 18:35:23 - kitanaqa.trainer.utils - INFO - Creating features from dataset file at ./example_outputs\n",
      "11/12/2020 18:35:23 - INFO - kitanaqa.trainer.utils -   Creating features from dataset file at ./example_outputs\n",
      "100%|██████████| 1/1 [00:00<00:00, 220.45it/s]\n",
      "convert squad examples to features: 100%|██████████| 10/10 [00:00<00:00, 61.84it/s]\n",
      "add example index and unique id: 100%|██████████| 10/10 [00:00<00:00, 64133.09it/s]\n",
      "11/12/2020 18:35:23 - kitanaqa.trainer.utils - INFO - Saving features into cached file ./example_outputs/cached_train_distilbert-base-uncased_512\n",
      "11/12/2020 18:35:23 - INFO - kitanaqa.trainer.utils -   Saving features into cached file ./example_outputs/cached_train_distilbert-base-uncased_512\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from kitanaqa.trainer.arguments import ModelArguments\n",
    "from kitanaqa.trainer.utils import load_and_cache_examples, build_flow\n",
    "from kitanaqa.trainer.run_pipeline import _setup\n",
    "hparams = {\n",
    "            \"model_type\" : \"distilbert\",\n",
    "            \"model_name_or_path\" : \"distilbert-base-uncased\",\n",
    "            \"output_dir\" : \"./example_outputs\",\n",
    "            \"cache_dir\" : \"./example_outputs\",\n",
    "            \"data_dir\" : \"./example_outputs\",\n",
    "            \"train_file_path\" : TRAIN_PATH,\n",
    "            \"predict_file_path\" : {\"dev-v1.1\": EVAL_PATH},\n",
    "            \"aug_file_path\" : None,\n",
    "            \"do_aug\" : False,\n",
    "            \"do_alum\" : False,\n",
    "            \"alpha\" : 5,\n",
    "            \"eps\" : 1e-4,\n",
    "            \"eta\" : 1e-5,\n",
    "            \"sigma\" : 1e-3,\n",
    "            \"do_train\" : True,\n",
    "            \"do_eval\" : True,\n",
    "            \"per_device_train_batch_size\" : 2,\n",
    "            \"per_device_eval_batch_size\" : 1,\n",
    "            \"gradient_accumulation_steps\" : 2,\n",
    "            \"eval_all_checkpoints\" : False,\n",
    "            \"num_train_epochs\" : 1,\n",
    "            \"max_steps\" : -1,\n",
    "            \"save_steps\" : 1,\n",
    "            \"seed\" : 512,\n",
    "            \"fp16\" : False\n",
    "}\n",
    "# Initialize args\n",
    "parser = HfArgumentParser(dataclass_types=[ModelArguments, TrainingArguments])\n",
    "model_args, training_args = parser.parse_dict(hparams)\n",
    "model, tokenizer, train_dataset = _setup(model_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 02:35:28] INFO - prefect.FlowRunner | Beginning Flow run for 'default'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:28 - INFO - prefect.FlowRunner -   Beginning Flow run for 'default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 02:35:28] INFO - prefect.TaskRunner | Task 'train': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:28 - INFO - prefect.TaskRunner -   Task 'train': Starting task run...\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ca70a7706a4a9591c32a2ab5dbf472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb53b624c7874e5a9e7ceadfd7304b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Iteration'), FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[2020-11-13 02:35:48] INFO - prefect.TaskRunner | Task 'train': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 02:35:48] INFO - prefect.TaskRunner | Task 'eval': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:48 - INFO - prefect.TaskRunner -   Task 'eval': Starting task run...\n",
      "11/12/2020 18:35:48 - kitanaqa.trainer.utils - WARNING - You are running a non-local model checkpoint. This may or may not be what you intended.\n",
      "11/12/2020 18:35:48 - WARNING - kitanaqa.trainer.utils -   You are running a non-local model checkpoint. This may or may not be what you intended.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
      "11/12/2020 18:35:50 - kitanaqa.trainer.utils - INFO - Creating features from dataset file at ./example_outputs\n",
      "11/12/2020 18:35:50 - INFO - kitanaqa.trainer.utils -   Creating features from dataset file at ./example_outputs\n",
      "100%|██████████| 1/1 [00:00<00:00, 171.97it/s]\n",
      "11/12/2020 18:35:50 - kitanaqa.trainer.utils - INFO - Evaluation Data is fetched for dev-v1.1.\n",
      "11/12/2020 18:35:50 - INFO - kitanaqa.trainer.utils -   Evaluation Data is fetched for dev-v1.1.\n",
      "convert squad examples to features: 100%|██████████| 10/10 [00:00<00:00, 67.12it/s]\n",
      "add example index and unique id: 100%|██████████| 10/10 [00:00<00:00, 60349.70it/s]\n",
      "11/12/2020 18:35:51 - kitanaqa.trainer.utils - INFO - Feature Extraction for Evaluation Data from dev-v1.1 is Finished.\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa.trainer.utils -   Feature Extraction for Evaluation Data from dev-v1.1 is Finished.\n",
      "11/12/2020 18:35:51 - kitanaqa.trainer.utils - INFO - Saving features into cached file ./example_outputs/cached_dev_distilbert-base-uncased_512\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa.trainer.utils -   Saving features into cached file ./example_outputs/cached_dev_distilbert-base-uncased_512\n",
      "11/12/2020 18:35:51 - kitanaqa.trainer.utils - INFO - Predict Sets are : dict_keys(['dev-v1.1'])\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa.trainer.utils -   Predict Sets are : dict_keys(['dev-v1.1'])\n",
      "11/12/2020 18:35:51 - kitanaqa - INFO - ***** Running evaluation distilbert-base-uncased *****\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa -   ***** Running evaluation distilbert-base-uncased *****\n",
      "11/12/2020 18:35:51 - kitanaqa - INFO -   Num examples = 10\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa -     Num examples = 10\n",
      "11/12/2020 18:35:51 - kitanaqa - INFO -   Batch size = 1\n",
      "11/12/2020 18:35:51 - INFO - kitanaqa -     Batch size = 1\n",
      "Evaluating: 100%|██████████| 10/10 [00:05<00:00,  1.90it/s]\n",
      "11/12/2020 18:35:56 - kitanaqa - INFO -   Evaluation done in total 5.265408 secs (0.526541 sec per example)\n",
      "11/12/2020 18:35:56 - INFO - kitanaqa -     Evaluation done in total 5.265408 secs (0.526541 sec per example)\n",
      "11/12/2020 18:35:56 - kitanaqa.trainer.utils - INFO - The evaluation for dev-v1.1 dataset is finished.\n",
      "11/12/2020 18:35:56 - INFO - kitanaqa.trainer.utils -   The evaluation for dev-v1.1 dataset is finished.\n",
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 02:35:56] INFO - prefect.TaskRunner | Task 'eval': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:56 - INFO - prefect.TaskRunner -   Task 'eval': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 02:35:56] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 18:35:56 - INFO - prefect.FlowRunner -   Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "# Construct a Flow pipeline consisting of a parent step (training) conditionally followed by a child step (evaluation)\n",
    "f = build_flow(\n",
    "            (model_args, training_args),\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset)\n",
    "if f:\n",
    "    f.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
