{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we show how a standard adversarial attack evaluation can be integrated into an end-to-end Flow pipeline. The example does not produce interesting results as it is run on a pretrained DistilBERT model for demo purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dummy train and test data from package\n",
    "import pkg_resources\n",
    "TRAIN_PATH = pkg_resources.resource_filename(\n",
    "            'kitanaqa', 'support/unittest-squad.json')\n",
    "EVAL_PATH = pkg_resources.resource_filename(\n",
    "            'kitanaqa', 'support/unittest-squad.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 17:46:31 - kitanaqa - WARNING - Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "11/12/2020 17:46:31 - WARNING - kitanaqa -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "11/12/2020 17:46:31 - kitanaqa - INFO - Training/evaluation parameters TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_17-46-31_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True)\n",
      "11/12/2020 17:46:31 - INFO - kitanaqa -   Training/evaluation parameters TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_17-46-31_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import HfArgumentParser, TrainingArguments\n",
    "from kitanaqa.trainer.arguments import ModelArguments\n",
    "from kitanaqa.trainer.utils import load_and_cache_examples, build_flow\n",
    "from kitanaqa.trainer.run_pipeline import _setup\n",
    "hparams = {\n",
    "            \"model_type\" : \"distilbert\",\n",
    "            \"model_name_or_path\" : \"distilbert-base-uncased\",\n",
    "            \"output_dir\" : \"./example_outputs\",\n",
    "            \"cache_dir\" : \"./example_outputs\",\n",
    "            \"data_dir\" : \"./example_outputs\",\n",
    "            \"train_file_path\" : TRAIN_PATH,\n",
    "            \"predict_file_path\" : {\"dev-v1.1\": EVAL_PATH},\n",
    "            \"aug_file_path\" : None,\n",
    "            \"do_aug\" : False,\n",
    "            \"do_alum\" : False,\n",
    "            \"alpha\" : 5,\n",
    "            \"eps\" : 1e-4,\n",
    "            \"eta\" : 1e-5,\n",
    "            \"sigma\" : 1e-3,\n",
    "            \"do_train\" : False,\n",
    "            \"do_adv_eval\" : True,  # Flag for adversarial attack instead of normal evaluation\n",
    "            \"do_eval\" : True,\n",
    "            \"per_device_train_batch_size\" : 2,\n",
    "            \"per_device_eval_batch_size\" : 1,\n",
    "            \"gradient_accumulation_steps\" : 2,\n",
    "            \"eval_all_checkpoints\" : False,\n",
    "            \"num_train_epochs\" : 1,\n",
    "            \"max_steps\" : -1,\n",
    "            \"save_steps\" : 1,\n",
    "            \"seed\" : 512,\n",
    "            \"fp16\" : False\n",
    "}\n",
    "# Initialize args\n",
    "parser = HfArgumentParser(dataclass_types=[ModelArguments, TrainingArguments])\n",
    "model_args, training_args = parser.parse_dict(hparams)\n",
    "model, tokenizer, train_dataset = _setup(model_args, training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 01:46:44] INFO - prefect.FlowRunner | Beginning Flow run for 'default'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 17:46:44 - INFO - prefect.FlowRunner -   Beginning Flow run for 'default'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 01:46:44] INFO - prefect.TaskRunner | Task 'eval': Starting task run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 17:46:44 - INFO - prefect.TaskRunner -   Task 'eval': Starting task run...\n",
      "11/12/2020 17:46:44 - kitanaqa.trainer.utils - WARNING - You are running a non-local model checkpoint. This may or may not be what you intended.\n",
      "11/12/2020 17:46:44 - WARNING - kitanaqa.trainer.utils -   You are running a non-local model checkpoint. This may or may not be what you intended.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/asisto/dev/Searchable/Research/KitanaQA/env/lib/python3.7/site-packages/transformers-3.1.0-py3.7.egg/transformers/trainer.py:247: FutureWarning: Passing `prediction_loss_only` as a keyword argument is deprecated and won't be possible in a future version. Use `args.prediction_loss_only` instead.\n",
      "  FutureWarning,\n",
      "You are instantiating a Trainer but Tensorboard is not installed. You should consider installing it.\n",
      "11/12/2020 17:46:46 - kitanaqa.trainer.utils - INFO - Loading features from cached file ./example_outputs/cached_dev_distilbert-base-uncased_512\n",
      "11/12/2020 17:46:46 - INFO - kitanaqa.trainer.utils -   Loading features from cached file ./example_outputs/cached_dev_distilbert-base-uncased_512\n",
      "11/12/2020 17:46:46 - kitanaqa.trainer.utils - INFO - Predict Sets are : dict_keys(['dev-v1.1'])\n",
      "11/12/2020 17:46:46 - INFO - kitanaqa.trainer.utils -   Predict Sets are : dict_keys(['dev-v1.1'])\n",
      "11/12/2020 17:46:46 - kitanaqa - INFO - ***** Running evaluation distilbert-base-uncased *****\n",
      "11/12/2020 17:46:46 - INFO - kitanaqa -   ***** Running evaluation distilbert-base-uncased *****\n",
      "11/12/2020 17:46:46 - kitanaqa - INFO -   Num examples = 10\n",
      "11/12/2020 17:46:46 - INFO - kitanaqa -     Num examples = 10\n",
      "11/12/2020 17:46:46 - kitanaqa - INFO -   Batch size = 1\n",
      "11/12/2020 17:46:46 - INFO - kitanaqa -     Batch size = 1\n",
      "Evaluating:   0%|          | 0/10 [00:00<?, ?it/s]/Users/asisto/dev/Searchable/Research/KitanaQA/src/kitanaqa/trainer/train.py:454: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  _delta = torch.tensor(_sample, requires_grad = True, device = self.args.device)\n",
      "Evaluating: 100%|██████████| 10/10 [00:24<00:00,  2.40s/it]\n",
      "11/12/2020 17:47:10 - kitanaqa - INFO -   Evaluation done in total 24.016870 secs (2.401687 sec per example)\n",
      "11/12/2020 17:47:10 - INFO - kitanaqa -     Evaluation done in total 24.016870 secs (2.401687 sec per example)\n",
      "11/12/2020 17:47:10 - kitanaqa.trainer.utils - INFO - The evaluation for dev-v1.1 dataset is finished.\n",
      "11/12/2020 17:47:10 - INFO - kitanaqa.trainer.utils -   The evaluation for dev-v1.1 dataset is finished.\n",
      "11/12/2020 17:47:10 - kitanaqa.trainer.utils - INFO - Results: {'dev-v1.1': {'uncased': {'model_args': ModelArguments(model_name_or_path='distilbert-base-uncased', train_file_path='/Users/asisto/dev/Searchable/Research/KitanaQA/src/kitanaqa/support/unittest-squad.json', predict_file_path={'dev-v1.1': '/Users/asisto/dev/Searchable/Research/KitanaQA/src/kitanaqa/support/unittest-squad.json'}, model_type='distilbert', aug_file_path=None, tokenizer_name_or_path=None, cache_dir='./example_outputs', do_lower_case=True, version_2_with_negative=False, null_score_diff_threshold=0.0, n_best_size=20, verbose_logging=False, overwrite_cache=False, max_seq_length=512, max_query_length=64, max_answer_length=30, doc_stride=128, freeze_embeds=False, do_aug=False, do_adv_eval=True, do_alum=False, eta=1e-05, alpha=5, alpha_final=None, alpha_schedule=None, eps=0.0001, sigma=0.001, K=1, data_dir='./example_outputs', eval_all_checkpoints=False), 'training_args': TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=True, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_17-46-31_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True), 'eval': OrderedDict([('exact', 0.0), ('f1', 16.338624338624335), ('total', 10), ('HasAns_exact', 0.0), ('HasAns_f1', 16.338624338624335), ('HasAns_total', 10), ('best_exact', 0.0), ('best_exact_thresh', 0.0), ('best_f1', 16.338624338624335), ('best_f1_thresh', 0.0)])}}}\n",
      "11/12/2020 17:47:10 - INFO - kitanaqa.trainer.utils -   Results: {'dev-v1.1': {'uncased': {'model_args': ModelArguments(model_name_or_path='distilbert-base-uncased', train_file_path='/Users/asisto/dev/Searchable/Research/KitanaQA/src/kitanaqa/support/unittest-squad.json', predict_file_path={'dev-v1.1': '/Users/asisto/dev/Searchable/Research/KitanaQA/src/kitanaqa/support/unittest-squad.json'}, model_type='distilbert', aug_file_path=None, tokenizer_name_or_path=None, cache_dir='./example_outputs', do_lower_case=True, version_2_with_negative=False, null_score_diff_threshold=0.0, n_best_size=20, verbose_logging=False, overwrite_cache=False, max_seq_length=512, max_query_length=64, max_answer_length=30, doc_stride=128, freeze_embeds=False, do_aug=False, do_adv_eval=True, do_alum=False, eta=1e-05, alpha=5, alpha_final=None, alpha_schedule=None, eps=0.0001, sigma=0.001, K=1, data_dir='./example_outputs', eval_all_checkpoints=False), 'training_args': TrainingArguments(output_dir='./example_outputs', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluate_during_training=False, prediction_loss_only=True, per_device_train_batch_size=2, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, warmup_steps=0, logging_dir='runs/Nov12_17-46-31_Aarons-MBP-7.attlocal.net', logging_first_step=False, logging_steps=500, save_steps=1, save_total_limit=None, no_cuda=False, seed=512, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=1000, past_index=-1, run_name=None, disable_tqdm=False, remove_unused_columns=True), 'eval': OrderedDict([('exact', 0.0), ('f1', 16.338624338624335), ('total', 10), ('HasAns_exact', 0.0), ('HasAns_f1', 16.338624338624335), ('HasAns_total', 10), ('best_exact', 0.0), ('best_exact_thresh', 0.0), ('best_f1', 16.338624338624335), ('best_f1_thresh', 0.0)])}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===alum_results:  OrderedDict([('exact', 0.0), ('f1', 16.338624338624335), ('total', 10), ('HasAns_exact', 0.0), ('HasAns_f1', 16.338624338624335), ('HasAns_total', 10), ('best_exact', 0.0), ('best_exact_thresh', 0.0), ('best_f1', 16.338624338624335), ('best_f1_thresh', 0.0)])\n",
      "[2020-11-13 01:47:10] INFO - prefect.TaskRunner | Task 'eval': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 17:47:10 - INFO - prefect.TaskRunner -   Task 'eval': finished task run for task with final state: 'Success'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-11-13 01:47:10] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/12/2020 17:47:10 - INFO - prefect.FlowRunner -   Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "# Construct a Flow pipeline consisting of a parent step (training) conditionally followed by a child step (evaluation)\n",
    "f = build_flow(\n",
    "            (model_args, training_args),\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            train_dataset=train_dataset)\n",
    "if f:\n",
    "    f.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
